# 演習解答欄

## 演習 1.1.2

作成している行
agent: 14
env: 12

呼び出されている変数・メソッド

agent:
* `act_and_train()`
* `act()`
* `get_statistics()`
* `stop_episode_and_train()`
* `stop_episode()`

env:
* `metadata`
* `reset()`
* `render()`
* `step()`

### 解説

より詳細なインタフェースについては，envについてはOpenAI gymの`gym.Env`を，agentについてはChainerRLの`chainerrl.agent.Agent`を参照してください．

## 演習 1.1.3

平均step数

(一例)
train: 47.02
test: 57.24

### 解説

`main.py`については，`main_complete.py`を参照してください．
test，train ともにおおよそ50 steps 程度になると思います．

## 演習 1.1.4

### 解説

`rulebase_agent_complete.py`を参照してください．
どっちが下でどっちが右か，どこに行ったらどういう観測が得られるのか，などを自分で確認する必要があります．
「ぶつかったら」という観測は得られませんが，解答のように自分のy座標を観測したり，座標が前回と等しいかどうかを観測したりすることで題意を満たすことができます．

## 演習 1.1.5

### 解説

`easymaze_env_complete.py`を参照してください．
`__init__()`メソッド内の`self.initial_maze`の初期値を変更することで実現できます．

これ以外にも，迷路の形をより大きなものにしたり，`(x, y) = (0, 2)`に到達したら負の報酬を与えて終了する迷路にしたりといった解法があります．

## 演習 1.1.6

### 解説

`easymaze_env_complete.py`を参照してください．
演習1.1.2で調べた`step()`や`render()`などに，`_step()`や`_render()`などがそれぞれ対応しています．

アンダースコアのないメソッドはスーパークラスである`gym.Env`で定義されており，適切なラップ処理を行った後にアンダースコアのあるメソッドが呼ばれるという形になっています．
これによって，`gym.Env`を継承したクラスを書く人が，いちいち`super()`を呼ばなくてもいいようになっています．

## 演習 1.1.7

### 解説

`table_q_agent_complete1.py`を参照してください．
`main.py`の変更は以下のとおりです．

* `agent = agents.RulebaseAgent(env, gpu_id)` の行をコメントアウト
* `agent = agents.TableQAgent(env, gpu_id)` の行のコメントアウトを解除

`obs`と`obs_key`の違いに注意してください．
環境から与えられる観測は，一般にそのままでは使えない場合があり，何かしらの形でハッシュ化できるように (あるいは，ニューラルネットワークに入れられるように) 前処理を行う必要があります．

今回のプログラムでは，OrderedDictで与えられる観測を，`observation_to_key()`メソッドでtupleにすることで，ハッシュテーブルのkeyとして使えるように加工しています．

`self.q_table`は観測をキーとして受け取り，行動の数 (4) を長さとするlistを返すハッシュとして実装されています．
これはDQNなどの構造を意識した形になっています．
教科書では
`Q(s, a)∈(実数)`
と書かれるQ学習ですが，実際に実装する際には
`Q(s)∈(長さAのvector)`
という形で実装することが多いです．すなわち，観測と行動の組に対して1つのQ値を与える実装ではなく，観測に対して可能な行動の数だけQ値を返す実装にすることが多いです．それはなぜかというと，ニューラルネットワークの学習を行う際に，教師信号は多ければ多いほどよく，逆に入力は少なければ少ないほど学習しやすいからです．
`Q(s, a)∈R`
ではなく
`Q(s)∈R^|A|`
とすることで，教師信号の空間を`|A|`倍にしつつ，入力の次元を`1/|A|`にできます．もちろんその分データの数は`1/|A|`になってしまいますが，差し引き`|A|`倍の得をしているというわけです．

## 演習 1.1.8

平均step数

(一例)
train: 7.56
test: 6.42

train first 10: 12.8
train last 10: 5.9

### 解説

最後の10 episodes の平均を出すのは難しくないので割愛します．

最初の10 > train > test >= 最後の10

という関係式が成り立っていれば正解です．

最初と最後で大きくstep数が変わっていることから，table-Q agent が正しく学習してより速くゴールできる道を見つけていることがわかります．
ただし，test 時であっても epsilon-greedy を用いているため，平均step 数が 4 にはならない点に注意が必要です．

## 演習 1.1.9

### 解説

`env_step(action)`の前後あたりに

```python
	print(agent.q_table_to_str())
```

または

```python
	render_buffer.prints(agent.q_table_to_str())
```

と書くことで観察できます．
`render_buffer`は単に描画間隔を揃えていい感じに出力してくれるだけのものなので，どちらでも構いませんが，下の書き方のほうがカクカクせずに観察できます．

割引率，学習率，迷路の報酬などを変えた場合ですが，まず学習率は変えても学習にあまり影響はありません．この値が真価を発揮するのは状態と行動によって次の状態が一意に定まらない場合です．そういった場合，ある状態からある行動を取ったときのQ値が何かしらの確率分布に基づく期待値になるので，2つ以上の結果を平均する必要があり，学習率が重要になってきます．

割引率に関しては，今回の迷路ではあまり関係ありませんが，1かそれ未満かだけは重要です．割引率が1の場合，例えば上下に動き回りながらゴールするような道と，ゴールにまっすぐ向かう道を，区別することができなくなってしまいます．その結果，すべてのアクションのQ値が1になってしまい，正しく学習することができません．学習率と同様，状態と行動によって次の状態が一意に定まらない場合には重要になってきます．

迷路の報酬を変えた場合ですが，例えば`(3, 0)`に1点の報酬を置くことで学習を速くすることができます (この際，一度報酬を取った後はもうもらえないように設計してください) ．これは，table-Q agent がランダムに行動する際，ノーヒントで`(3, 2)`にたどり着く確率よりも，`(0, 0)`からノーヒントで`(3, 0)`にたどり着き，報酬を得て`(3, 0)`に来ることを覚え，そこからノーヒントで`(3, 2)`にたどり着く確率のほうが高いからです．
このように，ゴールまで遠い状況では途中に報酬を置くことで学習を早めることができます．逆に言えば，途中に報酬のないゲームは学習が難しいということなので，報酬の設計は非常に重要な問題です．近年の研究では，この報酬をagent が自分で生成できるようなアルゴリズムも提案されています．

## 演習 1.1.10

平均step数

(一例)
train: 6.72
test: 4.0

### 解説

`table_q_agent_complete2.py`を参照してください．

testの平均step 数が4.0になっていることが重要です．すなわち，greedy に行動すれば確実に最短経路でゴールできるということです．

環境にランダム要素がないので，agent は 100 episodes ともすべて同じ行動を取ることになります．

## 演習 1.1.11

### 解説

環境がかなり単純なので，DQNのモデルを変えてもそこまで変化はないと思います．
CartPoleで同じ変更をして比較してみると良いでしょう．

## 演習 1.1.12

### 解説

CartPoleで全く学習できていない様子が観察できるはずです．

動画は見ていてわりと面白いので (特に正しく学習できているagentは)，ぜひローカルで実行してみてください．
ちなみに`ssh -X`というのを使うとサーバで起動した動画をローカルで見れるようなので，暇な人はやってみてください (TA未検証です)．

## 演習 1.1.13

### 解説

`dqn_agent_complete.py`，`dqn_model_complete.py`を参照してください．このモデルをちょうど 200episodes 学習させることで題意を満たすことを確認済みです．もちろんこれ以外にも条件を満たすモデルは無数に存在すると思います．

まず，最後の層のreluは要りません (出力するものはQ値なので)．引っ掛けです．
また，学習episode 数を増やすことで成績は一気に向上します．隠れ層のユニット数も50程度に増やすといいと思います．DQNをDoubleDQNに変更すると安定します．他にもいろいろなハイパーパラメータのチューニング方法があります．

10連続で200点を取るのは簡単ではありません．というかめっちゃ難しいです．それっぽいハイパーパラメータを見つけたら初期値ガチャをすることになるかもしれません．この課題のキモはどこを変えたら何が変わるのかを見極めることにあるので，必ずしも200点を取る必要はありませんが，色々試してみてください．

# 発展課題などについて

「こういうことがやってみたいんですけど…」
「こういう感じの問題ってありませんか？」
みたいな曖昧な質問でも構いませんので，どんどんTAに相談してください．

強化学習にこだわる必要もありませんし，逆に強化学習をどんどん使ってくれても構いません．

この実験の趣旨は深層学習をブラックボックスとしてじゃんじゃん使えるようになることなので，基礎的な部分の理解が浅いと感じても気にせず突き進んでくれればいいですが，もし気になるようであればそれはそれでTAが個別に対応しますし，ネットにもいい資料がたくさん上がっています．
